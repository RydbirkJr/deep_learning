{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning Setup #2\n",
    "\n",
    "In the following we will introduce our second deep Q-learning setup. It consists of:\n",
    "\n",
    "* ReplayMemory\n",
    "* Agent\n",
    "* The script to run the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReplayMemory\n",
    "\n",
    "This is the same replay memory as shown in setup #1. The data structure used for storing the last N transitions, used for sampling mini-batches from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, resolution, channels):\n",
    "        state_shape = (capacity, channels, resolution[0], resolution[1])\n",
    "        self.s1 = np.zeros(state_shape, dtype=np.float32)\n",
    "        self.s2 = np.zeros(state_shape, dtype=np.float32)\n",
    "        self.a = np.zeros(capacity, dtype=np.int32)\n",
    "        self.r = np.zeros(capacity, dtype=np.float32)\n",
    "        self.isterminal = np.zeros(capacity, dtype=np.bool_)\n",
    "\n",
    "        self.capacity = capacity\n",
    "        self.size = 0\n",
    "        self.pos = 0\n",
    "\n",
    "    def add_transition(self, s1, action, s2, isterminal, reward):\n",
    "        self.s1[self.pos] = s1\n",
    "        self.a[self.pos] = action\n",
    "        if not isterminal:\n",
    "            self.s2[self.pos] = s2\n",
    "        self.isterminal[self.pos] = isterminal\n",
    "        self.r[self.pos] = reward\n",
    "\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def get_sample(self, sample_size):\n",
    "        i = sample(range(0, self.size), sample_size)\n",
    "        return self.s1[i], self.a[i], self.s2[i], self.isterminal[i], self.r[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "The agent that contains the Q-function, and the methods used for preprocessing the data, training and validation. The main difference in this agent is the preprocessing and the use of the Q-hat-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pickle\n",
    "from random import randint, random, choice\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import skimage.color\n",
    "import skimage.transform\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from lasagne.layers import Conv2DLayer, InputLayer, DenseLayer, get_output, \\\n",
    "    get_all_params, get_all_param_values, set_all_param_values\n",
    "from lasagne.nonlinearities import rectify\n",
    "from lasagne.objectives import squared_error\n",
    "from lasagne.updates import rmsprop\n",
    "from tqdm import trange\n",
    "from replay_memory import ReplayMemory\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    \"\"\"\n",
    "    Reinforcement Learning Agent\n",
    "\n",
    "    This agent can learn to solve reinforcement learning tasks from\n",
    "    OpenAI Gym by applying the policy gradient method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, scale=1, discount_factor=0.99, learning_rate=0.00025,\n",
    "                 replay_memory_size=100000, batch_size=32, cropping=(0, 0, 0, 0), weights_file=None, rho=0.95,\n",
    "                 epsilon=0.01):\n",
    "\n",
    "        # Create the input variables\n",
    "        self.count = 0\n",
    "        s1 = T.tensor4(\"States\")\n",
    "        a = T.vector(\"Actions\", dtype=\"int32\")\n",
    "        q2 = T.vector(\"Next State's best Q-Value\")\n",
    "        r = T.vector(\"Rewards\")\n",
    "        isterminal = T.vector(\"IsTerminal\", dtype=\"int8\")\n",
    "\n",
    "        self.resolution = ((env.observation_space.shape[0] - cropping[0] - cropping[1]) * scale,\n",
    "                           (env.observation_space.shape[1] - cropping[2] - cropping[3]) * scale)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.batch_size = batch_size\n",
    "        self.actions = 3  # env.action_space\n",
    "        self.scale = scale\n",
    "        self.cropping = cropping\n",
    "        self.continue_training = False  # Overwritten if weights are given\n",
    "        self.channels = 3  # Channels because we stack frames\n",
    "\n",
    "        print(\"Resolution = \" + str(self.resolution))\n",
    "        print(\"Channels = \" + str(self.channels))\n",
    "\n",
    "        # Create replay memory which will store the transitions\n",
    "        self.memory = ReplayMemory(capacity=replay_memory_size, resolution=self.resolution, channels=self.channels)\n",
    "\n",
    "        self.dqn, _ = self.create_network(s1)\n",
    "        self.dqn_hat, self.conv_layers = self.create_network(s1)\n",
    "\n",
    "        if weights_file:\n",
    "            self.load_weights(weights_file)\n",
    "            self.continue_training = True\n",
    "\n",
    "        # Define the loss function\n",
    "        q = get_output(self.dqn)\n",
    "        q_hat = get_output(self.dqn_hat)\n",
    "\n",
    "        # target differs from q only for the selected action. The following means:\n",
    "        # target_Q(s,a) = r + gamma * max Q(s2,_) if isterminal else r\n",
    "        target_q = T.set_subtensor(q[T.arange(q.shape[0]), a], r + discount_factor * (1 - isterminal) * q2)\n",
    "        target_q_hat = T.set_subtensor(q_hat[T.arange(q_hat.shape[0]), a], r + discount_factor * (1 - isterminal) * q2)\n",
    "        loss = squared_error(q, target_q).mean()\n",
    "        loss_hat = squared_error(q, target_q_hat).mean()\n",
    "\n",
    "        # Update the parameters according to the computed gradient using RMSProp.\n",
    "        params = get_all_params(self.dqn, trainable=True)\n",
    "        updates = rmsprop(loss_hat, params, learning_rate, rho=rho, epsilon=epsilon)\n",
    "\n",
    "        # Compile the theano functions\n",
    "        print \"Compiling the network ...\"\n",
    "        self.fn_learn = theano.function([s1, q2, a, r, isterminal], loss_hat, updates=updates, name=\"learn_fn\")\n",
    "        self.fn_get_q_values = theano.function([s1], q_hat, name=\"eval_fn\")\n",
    "        self.fn_get_best_action = theano.function([s1], T.argmax(q), name=\"test_fn\")\n",
    "        print \"Network compiled.\"\n",
    "        self.env = env\n",
    "\n",
    "    def create_network(self, s1):\n",
    "        # policy network\n",
    "        l_in = InputLayer(shape=(None, self.channels, self.resolution[0], self.resolution[1]), input_var=s1)\n",
    "        l_conv1 = Conv2DLayer(l_in, num_filters=16, filter_size=[8, 8], nonlinearity=rectify, stride=4)\n",
    "        l_conv2 = Conv2DLayer(l_conv1, num_filters=32, filter_size=[4, 4], nonlinearity=rectify, stride=2)\n",
    "        l_conv3 = Conv2DLayer(l_conv2, num_filters=64, filter_size=[3, 3], nonlinearity=rectify, stride=1)\n",
    "        l_hid1 = DenseLayer(l_conv3, num_units=512, nonlinearity=rectify)\n",
    "        return DenseLayer(l_hid1, num_units=self.actions, nonlinearity=None), [l_conv1, l_conv2, l_conv3]\n",
    "\n",
    "    def load_weights(self, filename):\n",
    "        set_all_param_values(self.dqn, np.load(str(filename)))\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        return self.fn_get_best_action(state.reshape([1, self.channels, self.resolution[0], self.resolution[1]]))\n",
    "\n",
    "    '''\n",
    "        s2 has to have same shape as s1.\n",
    "    '''\n",
    "    def learn_from_transition(self, s1, a, s2, s2_isterminal, r):\n",
    "        \"\"\" Learns from a single transition (making use of replay memory).\n",
    "        s2 is ignored if s2_isterminal \"\"\"\n",
    "\n",
    "        # Get a random minibatch from the replay memory and learns from it.\n",
    "        if self.memory.size > self.batch_size:\n",
    "            s1, a, s2, isterminal, r = self.memory.get_sample(self.batch_size)\n",
    "            q2 = np.max(self.fn_get_q_values(s2), axis=1)\n",
    "            # the value of q2 is ignored in learn if s2 is terminal\n",
    "            self.fn_learn(s1, q2, a, r, isterminal)\n",
    "\n",
    "    def exploration_rate(self, epoch, epochs):\n",
    "        \"\"\"# Define exploration rate change over time\"\"\"\n",
    "        start_eps = 1.0\n",
    "        end_eps = 0.1\n",
    "        pct_random_rounds = 0.01\n",
    "\n",
    "        if self.continue_training:\n",
    "            start_eps = 0.30\n",
    "            pct_random_rounds = 0\n",
    "\n",
    "        const_eps_epochs = pct_random_rounds * epochs  # 10% of learning time\n",
    "        eps_decay_epochs = 0.9 * epochs  # 90% of learning time\n",
    "\n",
    "        if epoch < const_eps_epochs:\n",
    "            return start_eps\n",
    "        elif epoch < eps_decay_epochs:\n",
    "            # Linear decay\n",
    "            return start_eps - (epoch - const_eps_epochs) / \\\n",
    "                               (eps_decay_epochs - const_eps_epochs) * (start_eps - end_eps)\n",
    "        else:\n",
    "            return end_eps\n",
    "\n",
    "    def perform_learning_step(self, epoch, epochs, s1, no_learn_epochs):\n",
    "        \"\"\" Makes an action according to eps-greedy policy, observes the result\n",
    "        (next state, reward) and learns from the transition\"\"\"\n",
    "\n",
    "        # With probability eps make a random action.\n",
    "        eps = self.exploration_rate(epoch, epochs)\n",
    "        if random() <= eps:\n",
    "            a = randint(0, self.actions - 1)\n",
    "        else:\n",
    "            # Choose the best action according to the network.\n",
    "            a = self.get_best_action(s1)\n",
    "        (s2, reward, isterminal, _) = self.env.step(a + 1)\n",
    "\n",
    "        s2 = self.add_new_state_to_current(s1, self.preprocess(s2))\n",
    "\n",
    "        s3 = s2 if not isterminal else None\n",
    "        if isterminal:\n",
    "            x = 2  # TODO This doesn't do anything?\n",
    "\n",
    "        self.count += 1\n",
    "        if self.count % self.channels == 0:\n",
    "            self.count = 0\n",
    "            # Remember the transition that was just experienced.\n",
    "            self.memory.add_transition(s1, a, s2, isterminal, reward)\n",
    "            if epoch > no_learn_epochs:\n",
    "                self.learn_from_transition(s1, a, s3, isterminal, reward)\n",
    "\n",
    "        return s2, reward, isterminal\n",
    "\n",
    "    def preprocess(self, img):\n",
    "\n",
    "        # Crop\n",
    "        img = img[self.cropping[0]:len(img) - self.cropping[1], self.cropping[2]:len(img[0]) - self.cropping[3], 0:]\n",
    "\n",
    "        # Scaling\n",
    "        if self.scale != 1:\n",
    "            img = skimage.transform.rescale(img, self.scale)\n",
    "\n",
    "        # This is moved here because of the redef of channels.\n",
    "        img = skimage.color.rgb2gray(img)\n",
    "\n",
    "        img = img[np.newaxis, ...]\n",
    "        img = img.astype(np.float32)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def learn(self, render_training=False, render_test=False, learning_steps_per_epoch=10000,\n",
    "              test_episodes_per_epoch=5, epochs=100, max_test_steps=10000, no_learn_epochs=5):\n",
    "\n",
    "        print \"Starting the training!\"\n",
    "\n",
    "        train_results = []\n",
    "        test_results = []\n",
    "        best_result = -100  # Just low enough to ensure everything else will be better\n",
    "\n",
    "        time_start = time()\n",
    "        for epoch in range(epochs):\n",
    "            print \"\\nEpoch %d\\n-------\" % (epoch + 1)\n",
    "            eps = self.exploration_rate(epoch + 1, epochs)\n",
    "            print \"Eps = %.2f\" % eps\n",
    "            train_episodes_finished = 0\n",
    "            train_scores = []\n",
    "\n",
    "            print \"Training...\"\n",
    "            score = 0\n",
    "            s1 = self.env_reset()\n",
    "            # s1 = self.preprocess(s1)\n",
    "\n",
    "            # Because s1 contains the first 3 states\n",
    "            for learning_step in trange(learning_steps_per_epoch):\n",
    "                s2, reward, isterminal = self.perform_learning_step(epoch, epochs, s1, no_learn_epochs)\n",
    "                '''\n",
    "                a = self.get_best_action(s1)\n",
    "                (s2, reward, isterminal, _) = env.step(a)  # TODO: Check a\n",
    "                s2 = self.preprocess(s2) if not isterminal else None\n",
    "                '''\n",
    "                score += reward\n",
    "                s1 = s2  # s2 has been shifted into s1, and thus we just replace s1 here\n",
    "\n",
    "                if (render_training):\n",
    "                    self.env.render()\n",
    "                if isterminal:\n",
    "                    train_scores.append(score)\n",
    "                    s1 = self.env_reset()\n",
    "                    train_episodes_finished += 1\n",
    "                    score = 0\n",
    "\n",
    "            print \"%d training episodes played.\" % train_episodes_finished\n",
    "\n",
    "            train_scores = np.array(train_scores)\n",
    "\n",
    "            print \"Results: mean: %.1f±%.1f,\" % (train_scores.mean(), train_scores.std()), \\\n",
    "                \"min: %.1f,\" % train_scores.min(), \"max: %.1f,\" % train_scores.max()\n",
    "\n",
    "            train_results.append((train_scores.mean(), train_scores.std()))\n",
    "\n",
    "            print(\"Saving training results...\")\n",
    "            with open(\"train_results.txt\", \"w\") as train_result_file:\n",
    "                train_result_file.write(str(train_results))\n",
    "\n",
    "            test_scores = np.array(self.validate(test_episodes_per_epoch, max_test_steps, render_test))\n",
    "\n",
    "            if test_scores.max() > best_result:\n",
    "                print \"New best result. Storing weights.\"\n",
    "                best_result = test_scores.max()\n",
    "                pickle.dump(get_all_param_values(self.dqn), open('best_weights.dump', \"w\"))\n",
    "\n",
    "            print \"Results: mean: %.1f±%.1f,\" % (\n",
    "                test_scores.mean(), test_scores.std()), \"min: %.1f\" % test_scores.min(), \"max: %.1f\" % test_scores.max()\n",
    "\n",
    "            test_results.append((test_scores.mean(), test_scores.std()))\n",
    "\n",
    "            print(\"Saving test results...\")\n",
    "            with open(\"test_results.txt\", \"w\") as test_result_file:\n",
    "                test_result_file.write(str(test_results))\n",
    "\n",
    "            print \"Saving the network weights...\"\n",
    "            pickle.dump(get_all_param_values(self.dqn), open('weights.dump', \"w\"))\n",
    "\n",
    "            print \"Total elapsed time: %.2f minutes\" % ((time() - time_start) / 60.0)\n",
    "            print \"Best result so far: mean: %.1f, overall: %.1f\" % (\n",
    "            max([item[0] for item in test_results]), best_result)\n",
    "\n",
    "        # update dqn_hat at the end of every epoch\n",
    "        set_all_param_values(self.dqn_hat, get_all_param_values(self.dqn))\n",
    "\n",
    "    def env_reset(self):\n",
    "        s1 = self.env.reset()\n",
    "        s2, _, _, _ = self.env.step(choice([1, 2, 3]))\n",
    "        s3, _, _, _ = self.env.step(choice([1, 2, 3]))\n",
    "\n",
    "        res = np.zeros(shape=(self.channels, self.resolution[0], self.resolution[1]))\n",
    "        res = res.astype(np.float32)\n",
    "\n",
    "        res[0] = self.preprocess(s1)\n",
    "        res[1] = self.preprocess(s2)\n",
    "        res[2] = self.preprocess(s3)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def add_new_state_to_current(self, s1, s2):\n",
    "        res = np.zeros(shape=(self.channels, self.resolution[0], self.resolution[1]))\n",
    "        res = res.astype(np.float32)\n",
    "\n",
    "        res[0] = s1[1]\n",
    "        res[1] = s1[2]\n",
    "        res[2] = s2\n",
    "\n",
    "        return res\n",
    "\n",
    "    def validate(self, test_episodes_per_epoch=5, max_test_steps=10000, render_test=False):\n",
    "        print \"\\nTesting...\"\n",
    "        test_scores = []\n",
    "        for test_episode in trange(test_episodes_per_epoch):\n",
    "            s1 = self.env_reset()\n",
    "            score = 0\n",
    "            isterminal = False\n",
    "            frame = 0\n",
    "            while not isterminal and frame < max_test_steps:\n",
    "                a = self.get_best_action(s1)\n",
    "                (s2, reward, isterminal, _) = self.env.step(a + 1)\n",
    "\n",
    "                # I think this covers the statement bellow\n",
    "                if not isterminal:\n",
    "                    s2 = self.add_new_state_to_current(s1, self.preprocess(s2))\n",
    "                else:\n",
    "                    s2 = None\n",
    "\n",
    "                # s2 = self.preprocess(s2) if not isterminal else None\n",
    "\n",
    "                score += reward\n",
    "                s1 = s2\n",
    "                if (render_test):\n",
    "                    self.env.render()\n",
    "                frame += 1\n",
    "            test_scores.append(score)\n",
    "        return test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The script used for initiating the agent and contains several hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# init environment\n",
    "env = gym.make('Pong-v0')\n",
    "\n",
    "# init agent\n",
    "agent = Agent(env, batch_size=32, replay_memory_size=100000, scale=1, cropping=(30, 10, 6, 6), rho=0.95, epsilon=0.01)\n",
    "\n",
    "# train agent on the environment\n",
    "agent.learn(epochs=3000, render_training=False, render_test=False, learning_steps_per_epoch=10000, no_learn_epochs=5)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
