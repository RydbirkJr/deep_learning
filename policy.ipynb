{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Setup\n",
    "\n",
    "In the following we will introduce our policy gradient setup. It consists of:\n",
    "\n",
    "* Network\n",
    "* Agent\n",
    "* The script to run the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "The network is a self-sustained entity in this setup and consists only of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from lasagne.init import Constant\n",
    "from lasagne.layers import InputLayer, DenseLayer, Conv2DLayer, set_all_param_values\n",
    "from lasagne.nonlinearities import softmax, rectify\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, resolution, number_of_outputs, cropping, weights_file=None):\n",
    "\n",
    "        # symbolic variables for state, action, and advantage\n",
    "        self.sym_state = T.tensor4()\n",
    "        self.sym_action = T.vector(\"Actions\", dtype=\"int32\")\n",
    "        self.sym_advantage = T.vector(\"Advantages\", dtype=\"int32\")\n",
    "        self.sym_r = T.vector()\n",
    "        self.sym_q2 = T.vector()\n",
    "        self.shape = (None, 1,\n",
    "                      (resolution[0] - cropping[0] - cropping[1]),\n",
    "                      (resolution[1] - cropping[2] - cropping[3]))\n",
    "\n",
    "        self.cropping = cropping\n",
    "\n",
    "        # Same policy network as Deep Q\n",
    "        l_in = InputLayer(shape=self.shape, input_var=self.sym_state)\n",
    "        l_conv1 = Conv2DLayer(l_in, num_filters=16, filter_size=[8, 8], nonlinearity=rectify, stride=4)\n",
    "        l_conv2 = Conv2DLayer(l_conv1, num_filters=32, filter_size=[4, 4], nonlinearity=rectify, stride=2)\n",
    "        l_hid1 = DenseLayer(l_conv2, num_units=256, nonlinearity=rectify)\n",
    "        self.l_out = DenseLayer(incoming=l_hid1, W=Constant(1), num_units=number_of_outputs, nonlinearity=softmax,\n",
    "                           name='outputlayer')\n",
    "\n",
    "        if weights_file:\n",
    "            set_all_param_values(self.l_out, np.load(str(weights_file)))\n",
    "\n",
    "        # get network output\n",
    "        eval_out = lasagne.layers.get_output(self.l_out, {l_in: self.sym_state}, deterministic=True)\n",
    "\n",
    "        # get total number of timesteps\n",
    "        total_timesteps = self.sym_state.shape[0]\n",
    "\n",
    "        # loss function that we'll differentiate to get the policy gradient\n",
    "        loss = -T.log(eval_out[T.arange(total_timesteps), self.sym_action]).dot(self.sym_advantage) / total_timesteps\n",
    "\n",
    "        # learning_rate\n",
    "        learning_rate = T.fscalar()\n",
    "\n",
    "        # get trainable parameters in the network.\n",
    "        params = lasagne.layers.get_all_params(self.l_out, trainable=True)\n",
    "\n",
    "        # get gradients\n",
    "        grads = T.grad(loss, params)\n",
    "\n",
    "        # update function\n",
    "        updates = lasagne.updates.adam(grads, params, learning_rate=learning_rate)\n",
    "\n",
    "        print \"Compiling the network ...\"\n",
    "        self.f_train = theano.function(\n",
    "            [\n",
    "                self.sym_state,\n",
    "                self.sym_action,\n",
    "                self.sym_advantage,\n",
    "                learning_rate\n",
    "            ],\n",
    "            loss,\n",
    "            updates=updates,\n",
    "            allow_input_downcast=True\n",
    "        )\n",
    "        self.f_eval = theano.function([self.sym_state], eval_out, allow_input_downcast=True)\n",
    "        print \"Network compiled.\"\n",
    "\n",
    "    def train(self, all_states, all_actions, all_advantages, learning_rate):\n",
    "        return self.f_train(all_states, all_actions, all_advantages, learning_rate)\n",
    "\n",
    "    def evaluate(self, state):\n",
    "        return self.f_eval(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "The policy agent is shown bellow, heavily inspired by the agent given in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random, randint\n",
    "from lasagne.layers import get_all_param_values\n",
    "import pickle\n",
    "\n",
    "\n",
    "class AgentPolicy(object):\n",
    "    \n",
    "    def __init__(self, environment, network):\n",
    "        self.environment = environment\n",
    "        self.network = network\n",
    "\n",
    "    def learn(self,\n",
    "              epochs=100,\n",
    "              states_per_batch=10000,\n",
    "              time_limit=None,\n",
    "              learning_rate=0.01,\n",
    "              discount_factor=1.0,\n",
    "              early_stop=None):\n",
    "        \"\"\"\n",
    "        Learn the given environment by the policy gradient method.\n",
    "        \"\"\"\n",
    "        mean_train_rs = []\n",
    "        mean_val_rs = []\n",
    "        self.loss = []\n",
    "        self.epochs = epochs\n",
    "        self.epoch = 1\n",
    "        print \"Start training using %d epochs, %d states per batch, %d timelimit, %1.5f learning rate\" % (epochs,\n",
    "            states_per_batch, time_limit, learning_rate)\n",
    "\n",
    "        best_result = -100  # Just low enough to ensure everything else will be better\n",
    "\n",
    "        for epoch in xrange(epochs):\n",
    "            self.epoch = epoch\n",
    "            # 1. collect trajectories until we have at least states_per_batch total timesteps\n",
    "            trajectories = []\n",
    "            total_trajectories = 0\n",
    "            total_games = 0\n",
    "\n",
    "            while total_trajectories < states_per_batch:\n",
    "                print total_trajectories,\"/\",states_per_batch\n",
    "                trajectory = self.get_trajectory(time_limit, deterministic=False)\n",
    "                trajectories.append(trajectory)\n",
    "                total_trajectories += len(trajectory[\"reward\"])\n",
    "                total_games += 1\n",
    "\n",
    "            all_states = np.concatenate([trajectory[\"state\"] for trajectory in trajectories])\n",
    "\n",
    "            # 2. compute cumulative discounted rewards (returns)\n",
    "            rewards = [self._cumulative_discount(trajectory[\"reward\"], discount_factor) for trajectory in trajectories]\n",
    "            maxlen = max(len(reward) for reward in rewards)\n",
    "            padded_rewards = [np.concatenate([reward, np.zeros(maxlen - len(reward))]) for reward in rewards]\n",
    "\n",
    "            # 3. compute time-dependent baseline\n",
    "            baseline = np.mean(padded_rewards, axis=0)\n",
    "\n",
    "            # 4. compute advantages\n",
    "            advs = [reward - baseline[:len(reward)] for reward in rewards]\n",
    "            all_actions = np.concatenate([trajectory[\"action\"] for trajectory in trajectories])\n",
    "            all_advantages = np.concatenate(advs)\n",
    "\n",
    "            print 'Updating network...'\n",
    "            # 5. do policy gradient update step\n",
    "            loss = self.network.train(all_states, all_actions, all_advantages, learning_rate)\n",
    "\n",
    "            train_rs = np.array([trajectory[\"reward\"].sum() for trajectory in trajectories])  # trajectory total rewards\n",
    "            # eplens = np.array([len(trajectory[\"reward\"]) for trajectory in trajectories])  # trajectory lengths\n",
    "\n",
    "            print(\"Saving training results...\")\n",
    "            with open(\"train_results.txt\", \"w\") as train_result_file:\n",
    "                train_result_file.write(str((train_rs.mean())))\n",
    "\n",
    "            print \"\\nTesting...\"\n",
    "            # compute validation reward\n",
    "            val_reward = np.array(\n",
    "                [self.get_trajectory(epoch, epochs, time_limit, deterministic=True, render=False)['reward'].sum() for _ in range(1)]\n",
    "            )\n",
    "\n",
    "            # update stats\n",
    "            mean_train_rs.append(train_rs.mean())\n",
    "            mean_val_rs.append(val_reward.mean())\n",
    "            self.loss.append(loss)\n",
    "\n",
    "            if val_reward.max() > best_result:\n",
    "                print \"New best result. Storing weights.\"\n",
    "                best_result = val_reward.max()\n",
    "                pickle.dump(get_all_param_values(self.network.l_out), open('best_weights.dump', \"w\"))\n",
    "\n",
    "            print(\"Saving test results...\")\n",
    "            with open(\"test_results.txt\", \"w\") as test_result_file:\n",
    "                test_result_file.write(str((val_reward.mean())))\n",
    "\n",
    "            print \"Saving the network weights...\"\n",
    "            pickle.dump(get_all_param_values(self.network.l_out), open('weights.dump', \"w\"))\n",
    "\n",
    "            # print stats\n",
    "            print '%3d mean_train_r: %6.2f mean_val_r: %6.2f loss: %f games played: %3d' % (\n",
    "                epoch + 1, train_rs.mean(), val_reward.mean(), loss, total_games)\n",
    "\n",
    "            # check for early stopping: true if the validation reward has not changed in n_early_stop epochs\n",
    "            if early_stop and len(mean_val_rs) >= early_stop and \\\n",
    "                    all([x == mean_val_rs[-1] for x in mean_val_rs[-early_stop:-1]]):\n",
    "                break\n",
    "\n",
    "    def get_trajectory(self, epoch, epochs, time_limit=None, deterministic=True, render=False):\n",
    "        \"\"\"\n",
    "        Compute state by iteratively evaluating the agent policy on the environment.\n",
    "        \"\"\"\n",
    "        time_limit = time_limit or self.environment.spec.timestep_limit\n",
    "\n",
    "        # Get stacked initial state\n",
    "        s1 = self.environment.reset()\n",
    "\n",
    "        trajectory = {'state': [], 'action': [], 'reward': []}\n",
    "\n",
    "        for _ in xrange(time_limit):\n",
    "            action = self.get_action(epoch, epochs, s1, deterministic)\n",
    "            (s2, reward, done, _) = self.environment.step(action + 1)\n",
    "\n",
    "            if render:\n",
    "                self.environment.render()\n",
    "\n",
    "            s2 = self._state_reshape(s2)\n",
    "\n",
    "            trajectory['state'].append(s2)\n",
    "            trajectory['action'].append(action)\n",
    "            trajectory['reward'].append(reward)\n",
    "\n",
    "            if done: break\n",
    "\n",
    "        return {'state': np.array(trajectory['state']),\n",
    "                'action': np.array(trajectory['action']),\n",
    "                'reward': np.array(trajectory['reward'])}\n",
    "\n",
    "    def get_action(self, epoch, epochs, state, deterministic=True):\n",
    "        \"\"\"\n",
    "        Evaluate the agent policy to choose an action, a, given state, s.\n",
    "        \"\"\"\n",
    "\n",
    "        if deterministic:\n",
    "            # choose action with highest probability\n",
    "            action_probabilities = self.network.evaluate(np.expand_dims(state, 0))\n",
    "            action = action_probabilities.argmax()\n",
    "\n",
    "        else:\n",
    "            exp_rate = self.exploration_rate(epoch, epochs)\n",
    "\n",
    "            if random() <= exp_rate:\n",
    "                action = randint(0, 2)\n",
    "            else:\n",
    "                # Choose the best action according to the network.\n",
    "                action_probabilities = self.network.evaluate(np.expand_dims(state, 0))\n",
    "                action = action_probabilities.argmax()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def exploration_rate(self, epoch, epochs):\n",
    "        \"\"\"# Define exploration rate change over time\"\"\"\n",
    "        start_eps = 1.0\n",
    "        end_eps = 0.1\n",
    "        const_eps_epochs = 0.01 * epochs  # 10% of learning time\n",
    "        eps_decay_epochs = 0.9 * epochs  # 60% of learning time\n",
    "\n",
    "        if epoch < const_eps_epochs:\n",
    "            return start_eps\n",
    "        elif epoch < eps_decay_epochs:\n",
    "            # Linear decay\n",
    "            return start_eps - (epoch - const_eps_epochs) / \\\n",
    "                               (eps_decay_epochs - const_eps_epochs) * (start_eps - end_eps)\n",
    "        else:\n",
    "            return end_eps\n",
    "\n",
    "    def _cumulative_discount(self, reward, gamma):\n",
    "        \"\"\"\n",
    "        Compute the cumulative discounted rewards.\n",
    "        \"\"\"\n",
    "        reward_out = np.zeros(len(reward), 'float64')\n",
    "        reward_out[-1] = reward[-1]\n",
    "        for i in reversed(xrange(len(reward) - 1)):\n",
    "            reward_out[i] = reward[i] + gamma * reward_out[i + 1]\n",
    "        return reward_out\n",
    "\n",
    "    def get_state_shape(self):\n",
    "        return self.network.shape[1], self.network.shape[2], self.network.shape[3]\n",
    "\n",
    "    def _state_reshape(self, state):\n",
    "        img = Image.fromarray(state, 'RGB').convert('L')\n",
    "        size = (self.network.shape[2], self.network.shape[2])\n",
    "        img.thumbnail(size, Image.ANTIALIAS)\n",
    "        return np.expand_dims(np.array(img), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "All there is left is simply to start running the training with the given hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# init environment\n",
    "env = gym.make('Pong-v0')\n",
    "\n",
    "cropping = (30, 10, 6, 6)\n",
    "\n",
    "network = Network(resolution=env.observation_space.shape, number_of_outputs=3, cropping=cropping)\n",
    "print 'Completed network'\n",
    "agent = AgentPolicy(env, network)\n",
    "print 'Completed policy'\n",
    "\n",
    "# train agent on the environment\n",
    "agent.learn(\n",
    "    epochs=2500,\n",
    "    learning_rate=0.00025,\n",
    "    discount_factor=0.99,\n",
    "    states_per_batch=10000,\n",
    "    time_limit=10000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
