{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning Setup #1\n",
    "\n",
    "In the following we will introduce our first deep Q-learning setup. It consists of:\n",
    "\n",
    "* ReplayMemory\n",
    "* Agent\n",
    "* The script to run the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReplayMemory\n",
    "\n",
    "The data structure used for storing the last N transitions, used for sampling mini-batches from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, resolution, channels):\n",
    "        state_shape = (capacity, channels, resolution[0], resolution[1])\n",
    "        self.s1 = np.zeros(state_shape, dtype=np.float32)\n",
    "        self.s2 = np.zeros(state_shape, dtype=np.float32)\n",
    "        self.a = np.zeros(capacity, dtype=np.int32)\n",
    "        self.r = np.zeros(capacity, dtype=np.float32)\n",
    "        self.isterminal = np.zeros(capacity, dtype=np.bool_)\n",
    "\n",
    "        self.capacity = capacity\n",
    "        self.size = 0\n",
    "        self.pos = 0\n",
    "\n",
    "    def add_transition(self, s1, action, s2, isterminal, reward):\n",
    "        self.s1[self.pos] = s1\n",
    "        self.a[self.pos] = action\n",
    "        if not isterminal:\n",
    "            self.s2[self.pos] = s2\n",
    "        self.isterminal[self.pos] = isterminal\n",
    "        self.r[self.pos] = reward\n",
    "\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def get_sample(self, sample_size):\n",
    "        i = sample(range(0, self.size), sample_size)\n",
    "        return self.s1[i], self.a[i], self.s2[i], self.isterminal[i], self.r[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "The agent that contains the Q-function, and the methods used for preprocessing the data, training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pickle\n",
    "from random import randint, random\n",
    "from time import time\n",
    "import numpy as np\n",
    "import skimage.color\n",
    "import skimage.transform\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from lasagne.init import HeUniform, Constant\n",
    "from lasagne.layers import Conv2DLayer, InputLayer, DenseLayer, get_output, \\\n",
    "    get_all_params, get_all_param_values, set_all_param_values\n",
    "from lasagne.nonlinearities import rectify\n",
    "from lasagne.objectives import squared_error\n",
    "from lasagne.updates import rmsprop\n",
    "from tqdm import trange\n",
    "\n",
    "from replay_memory import ReplayMemory\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    \"\"\"\n",
    "    Reinforcement Learning Agent\n",
    "\n",
    "    This agent can learn to solve reinforcement learning tasks from\n",
    "    OpenAI Gym by applying the policy gradient method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, colors=True, scale=1, discount_factor=0.99, learning_rate=0.00025,\n",
    "                 replay_memory_size=100000, batch_size=32, cropping=(0, 0, 0, 0), weights_file=None):\n",
    "\n",
    "        # Create the input variables\n",
    "        s1 = T.tensor4(\"States\")\n",
    "        a = T.vector(\"Actions\", dtype=\"int32\")\n",
    "        q2 = T.vector(\"Next State's best Q-Value\")\n",
    "        r = T.vector(\"Rewards\")\n",
    "        isterminal = T.vector(\"IsTerminal\", dtype=\"int8\")\n",
    "\n",
    "        # Set field values\n",
    "        if colors:\n",
    "            self.channels = 3\n",
    "        else:\n",
    "            self.channels = 1\n",
    "        self.resolution = ((env.observation_space.shape[0] - cropping[0] - cropping[1]) * scale, \\\n",
    "                           (env.observation_space.shape[1] - cropping[2] - cropping[3]) * scale)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.batch_size = batch_size\n",
    "        self.actions = 3 # hardcoded\n",
    "        self.scale = scale\n",
    "        self.cropping = cropping\n",
    "\n",
    "        print(\"Resolution = \" + str(self.resolution))\n",
    "        print(\"Channels = \" + str(self.channels))\n",
    "\n",
    "        # Create replay memory which will store the transitions\n",
    "        self.memory = ReplayMemory(capacity=replay_memory_size, resolution=self.resolution, channels=self.channels)\n",
    "\n",
    "        # policy network\n",
    "        l_in = InputLayer(shape=(None, self.channels, self.resolution[0], self.resolution[1]), input_var=s1)\n",
    "        l_conv1 = Conv2DLayer(l_in, num_filters=16, filter_size=[8, 8], nonlinearity=rectify, W=HeUniform(\"relu\"),\n",
    "                              b=Constant(.1), stride=4)\n",
    "        l_conv2 = Conv2DLayer(l_conv1, num_filters=32, filter_size=[4, 4], nonlinearity=rectify, W=HeUniform(\"relu\"),\n",
    "                              b=Constant(.1), stride=2)\n",
    "        l_conv3 = Conv2DLayer(l_conv2, num_filters=64, filter_size=[3, 3], nonlinearity=rectify, W=HeUniform(\"relu\"),\n",
    "                              b=Constant(.1), stride=1)\n",
    "        l_hid1 = DenseLayer(l_conv3, num_units=256, nonlinearity=rectify, W=HeUniform(\"relu\"), b=Constant(.1))\n",
    "        self.dqn = DenseLayer(l_hid1, num_units=self.actions, nonlinearity=None)\n",
    "\n",
    "        if weights_file:\n",
    "            self.load_weights(weights_file)\n",
    "\n",
    "        # Define the loss function\n",
    "        q = get_output(self.dqn)\n",
    "        # target differs from q only for the selected action. The following means:\n",
    "        # target_Q(s,a) = r + gamma * max Q(s2,_) if isterminal else r\n",
    "        target_q = T.set_subtensor(q[T.arange(q.shape[0]), a], r + discount_factor * (1 - isterminal) * q2)\n",
    "        loss = squared_error(q, target_q).mean()\n",
    "\n",
    "        # Update the parameters according to the computed gradient using RMSProp.\n",
    "        params = get_all_params(self.dqn, trainable=True)\n",
    "        updates = rmsprop(loss, params, learning_rate)\n",
    "\n",
    "        # Compile the theano functions\n",
    "        print \"Compiling the network ...\"\n",
    "        self.fn_learn = theano.function([s1, q2, a, r, isterminal], loss, updates=updates, name=\"learn_fn\")\n",
    "        self.fn_get_q_values = theano.function([s1], q, name=\"eval_fn\")\n",
    "        self.fn_get_best_action = theano.function([s1], T.argmax(q), name=\"test_fn\")\n",
    "        print \"Network compiled.\"\n",
    "        self.env = env\n",
    "\n",
    "    def load_weights(self, filename):\n",
    "        set_all_param_values(self.dqn, np.load(str(filename)))\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        return self.fn_get_best_action(state.reshape([1, self.channels, self.resolution[0], self.resolution[1]]))\n",
    "\n",
    "    def learn_from_transition(self, s1, a, s2, s2_isterminal, r):\n",
    "        \"\"\" Learns from a single transition (making use of replay memory).\n",
    "        s2 is ignored if s2_isterminal \"\"\"\n",
    "\n",
    "        # Remember the transition that was just experienced.\n",
    "        self.memory.add_transition(s1, a, s2, s2_isterminal, r)\n",
    "\n",
    "        # Get a random minibatch from the replay memory and learns from it.\n",
    "        if self.memory.size > self.batch_size:\n",
    "            s1, a, s2, isterminal, r = self.memory.get_sample(self.batch_size)\n",
    "            q2 = np.max(self.fn_get_q_values(s2), axis=1)\n",
    "            # the value of q2 is ignored in learn if s2 is terminal\n",
    "            self.fn_learn(s1, q2, a, r, isterminal)\n",
    "\n",
    "    def exploration_rate(self, epoch, epochs):\n",
    "        \"\"\"# Define exploration rate change over time\"\"\"\n",
    "        start_eps = 1.0\n",
    "        end_eps = 0.1\n",
    "        const_eps_epochs = 0.01 * epochs  # 10% of learning time\n",
    "        eps_decay_epochs = 0.9 * epochs  # 60% of learning time\n",
    "\n",
    "        if epoch < const_eps_epochs:\n",
    "            return start_eps\n",
    "        elif epoch < eps_decay_epochs:\n",
    "            # Linear decay\n",
    "            return start_eps - (epoch - const_eps_epochs) / \\\n",
    "                               (eps_decay_epochs - const_eps_epochs) * (start_eps - end_eps)\n",
    "        else:\n",
    "            return end_eps\n",
    "\n",
    "    def perform_learning_step(self, epoch, epochs, s1):\n",
    "        \"\"\" Makes an action according to eps-greedy policy, observes the result\n",
    "        (next state, reward) and learns from the transition\"\"\"\n",
    "\n",
    "        # With probability eps make a random action.\n",
    "        eps = self.exploration_rate(epoch, epochs)\n",
    "        if random() <= eps:\n",
    "            a = randint(0, self.actions - 1)\n",
    "        else:\n",
    "            # Choose the best action according to the network.\n",
    "            a = self.get_best_action(s1)\n",
    "        (s2, reward, isterminal, _) = self.env.step(a+1)  # TODO: Check a\n",
    "        s2 = self.preprocess(s2)\n",
    "        s3 = s2 if not isterminal else None\n",
    "        if isterminal:\n",
    "            x = 2\n",
    "        self.learn_from_transition(s1, a, s3, isterminal, reward)\n",
    "\n",
    "        return s2, reward, isterminal\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        # Crop\n",
    "        img = img[self.cropping[0]:len(img) - self.cropping[1], self.cropping[2]:len(img[0]) - self.cropping[3], 0:]\n",
    "\n",
    "        # Scaling\n",
    "        if self.scale != 1:\n",
    "            img = skimage.transform.rescale(img, self.scale)\n",
    "\n",
    "        # Grayscale\n",
    "        if self.channels == 1:\n",
    "            img = skimage.color.rgb2gray(img)\n",
    "            img = img[np.newaxis, ...]\n",
    "        else:\n",
    "            img = img.reshape(self.channels, self.resolution[0], self.resolution[1])\n",
    "        img = img.astype(np.float32)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def learn(self, render_training=False, render_test=False, learning_steps_per_epoch=10000, \\\n",
    "              test_episodes_per_epoch=1, epochs=100, max_test_steps=2000):\n",
    "\n",
    "        print \"Starting the training!\"\n",
    "\n",
    "        train_results = []\n",
    "        test_results = []\n",
    "\n",
    "        time_start = time()\n",
    "        for epoch in range(epochs):\n",
    "            print \"\\nEpoch %d\\n-------\" % (epoch + 1)\n",
    "            eps = self.exploration_rate(epoch + 1, epochs)\n",
    "            print \"Eps = %.2f\" % eps\n",
    "            train_episodes_finished = 0\n",
    "            train_scores = []\n",
    "\n",
    "            print \"Training...\"\n",
    "            s1 = self.env.reset()\n",
    "            s1 = self.preprocess(s1)\n",
    "            score = 0\n",
    "            for learning_step in trange(learning_steps_per_epoch):\n",
    "                s2, reward, isterminal = self.perform_learning_step(epoch, epochs, s1)\n",
    "                '''\n",
    "                a = self.get_best_action(s1)\n",
    "                (s2, reward, isterminal, _) = env.step(a)  # TODO: Check a\n",
    "                s2 = self.preprocess(s2) if not isterminal else None\n",
    "                '''\n",
    "                score += reward\n",
    "                s1 = s2\n",
    "                if (render_training):\n",
    "                    self.env.render()\n",
    "                if isterminal:\n",
    "                    train_scores.append(score)\n",
    "                    s1 = self.env.reset()\n",
    "                    s1 = self.preprocess(s1)\n",
    "                    train_episodes_finished += 1\n",
    "                    score = 0\n",
    "\n",
    "            print \"%d training episodes played.\" % train_episodes_finished\n",
    "\n",
    "            train_scores = np.array(train_scores)\n",
    "\n",
    "            print \"Results: mean: %.1f±%.1f,\" % (train_scores.mean(), train_scores.std()), \\\n",
    "                \"min: %.1f,\" % train_scores.min(), \"max: %.1f,\" % train_scores.max()\n",
    "\n",
    "            train_results.append((train_scores.mean(), train_scores.std()))\n",
    "\n",
    "            print(\"Saving training results...\")\n",
    "            with open(\"train_results.txt\", \"w\") as train_result_file:\n",
    "                train_result_file.write(str(train_results))\n",
    "\n",
    "            test_scores = np.array(self.validate(test_episodes_per_epoch, max_test_steps, render_test))\n",
    "\n",
    "            print \"Results: mean: %.1f±%.1f,\" % (\n",
    "                test_scores.mean(), test_scores.std()), \"min: %.1f\" % test_scores.min(), \"max: %.1f\" % test_scores.max()\n",
    "\n",
    "            test_results.append((test_scores.mean(), test_scores.std()))\n",
    "\n",
    "            print(\"Saving test results...\")\n",
    "            with open(\"test_results.txt\", \"w\") as test_result_file:\n",
    "                test_result_file.write(str(test_results))\n",
    "\n",
    "            print \"Saving the network weigths...\"\n",
    "            pickle.dump(get_all_param_values(self.dqn), open('weights.dump', \"w\"))\n",
    "\n",
    "            print \"Total elapsed time: %.2f minutes\" % ((time() - time_start) / 60.0)\n",
    "\n",
    "    def validate(self, test_episodes_per_epoch=1, max_test_steps=2000, render_test=False):\n",
    "        print \"\\nTesting...\"\n",
    "        test_scores = []\n",
    "        for test_episode in trange(test_episodes_per_epoch):\n",
    "            s1 = self.env.reset()\n",
    "            s1 = self.preprocess(s1)\n",
    "            score = 0\n",
    "            isterminal = False\n",
    "            frame = 0\n",
    "            while not isterminal and frame < max_test_steps:\n",
    "                a = self.get_best_action(s1)\n",
    "                (s2, reward, isterminal, _) = self.env.step(a+1)  # TODO: Check a\n",
    "                s2 = self.preprocess(s2) if not isterminal else None\n",
    "                score += reward\n",
    "                s1 = s2\n",
    "                if (render_test):\n",
    "                    self.env.render()\n",
    "                frame += 1\n",
    "            test_scores.append(score)\n",
    "        return test_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The script used for initiating the agent and contains several hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# init environment\n",
    "env = gym.make('Pong-v0')\n",
    "\n",
    "# init agent\n",
    "agent = Agent(env, batch_size=64, replay_memory_size=312500, colors=False, scale=1, cropping=(30, 10, 6, 6))\n",
    "\n",
    "# train agent on the environment\n",
    "agent.learn(epochs=500, render_training=False, render_test=False)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
